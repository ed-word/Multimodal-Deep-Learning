{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24aa309",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # GLOVE\n",
    "\n",
    "# # Initialise config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18188b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "get_ipython().system('pip install nltk')\n",
    "get_ipython().system('pip install --upgrade tensorflow_hub')\n",
    "get_ipython().system('pip install --upgrade pydot')\n",
    "get_ipython().system('pip install --upgrade graphviz')\n",
    "get_ipython().system('pip install --upgrade matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8776ed65",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import tensorflow.keras as keras\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "import scipy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "#HYPERPARAMETERS\n",
    "BATCH_SIZE=64\n",
    "EPOCHS=20\n",
    "\n",
    "new_shape = (150,150)\n",
    "\n",
    "\n",
    "max_words = 15\n",
    "\n",
    "\n",
    "\n",
    "labels_dict = {\n",
    "    0: 'missing',\n",
    "    1: 'not-easy-fraud',\n",
    "    2: 'replaced'\n",
    "}\n",
    "\n",
    "rev_labels_dict = {\n",
    "    'missing': 0,\n",
    "    'not-easy-fraud': 1,\n",
    "    'replaced': 2\n",
    "}\n",
    "\n",
    "\n",
    "'''\n",
    "Need to revise these values after imbalanced augmentation\n",
    "'''\n",
    "num_records = {\n",
    "    'missing': 1000,\n",
    "    'not-easy-fraud': 30000,\n",
    "    'replaced': 350\n",
    "}\n",
    "\n",
    "num_augmentations = {\n",
    "    'missing': 5,\n",
    "    'not-easy-fraud': 3,\n",
    "    'replaced': 10\n",
    "}\n",
    "\n",
    "num_records_after_augmentation = {\n",
    "    'missing': num_records['missing']*num_augmentations['missing'],\n",
    "    'not-easy-fraud': num_records['not-easy-fraud']*num_augmentations['not-easy-fraud'],\n",
    "    'replaced': num_records['replaced']*num_augmentations['replaced']\n",
    "}\n",
    "\n",
    "# Majority class (not fraud)\n",
    "records = num_records_after_augmentation['not-easy-fraud']\n",
    "total_records = sum([num_records_after_augmentation[key] for key in num_records_after_augmentation])\n",
    "missing_records = num_records_after_augmentation['missing']\n",
    "replaced_records = num_records_after_augmentation['replaced']\n",
    "\n",
    "print(records)\n",
    "print(total_records)\n",
    "print(num_records_after_augmentation['missing'])\n",
    "print(num_records_after_augmentation['replaced'])\n",
    "\n",
    "class_weight = {\n",
    "    0: records/missing_records,\n",
    "    1: 1,\n",
    "    2: records/replaced_records\n",
    "}\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae3577",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# !pip install --upgrade tensorflow_hub\n",
    "# !pip install --upgrade pydot\n",
    "# !pip install --upgrade graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c60b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf24946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fbe0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e86436",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b162247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Data loading and Analysis\n",
    "\n",
    "# ## Download the files\n",
    "# <a id='cell:data_description'></a>\n",
    "# Please download the data from the following [WorkDocs link](https://amazon.awsapps.com/workdocs/index.html#/folder/4aefea54a65e7377cd884e1a54f341dc62aec676d88ac31127f4d88e75d63f28). It includes the following files:\n",
    "# - images.part_aa\n",
    "# - images.part_ab\n",
    "# - images.part_ac\n",
    "# - images.part_ad\n",
    "# - train_data.csv\n",
    "# - test_features.csv\n",
    "# \n",
    "# The first 4 files are part files which when concatenated lead to a tar ball containing the image files. The `train_data.csv` contains the training data, while the `test_features.csv` contains the test data (used for both the Public and Private Leaderboard) and thus **does not** include the labels.\n",
    "\n",
    "# ## Download google universal encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2aac31",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#download the model to local so it can be used again and again\n",
    "# !mkdir universalencoder\n",
    "# Download the module, and uncompress it to the destination folder. \n",
    "# !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed\" | tar -zxvC universalencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Let's see what kind of data we're dealing with\n",
    "train_data = pd.read_csv('WW Returns/train_data.csv')\n",
    "train_data.tail(100)\n",
    "\n",
    "\n",
    "# ## Getting the images\n",
    "# The images are present in a tar file which has been split into part files for making the download/upload convenient. First make sure that you have all the 4 files mentioned [here](#cell:data_description). Run the following cell to stitch the part files together and get the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850fec2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Let's try to look at an image for a return!\n",
    "get_ipython().magic('pylab inline')\n",
    "\n",
    "index = -3\n",
    "image_id, label = train_data.iloc[index]['image_id'], train_data.iloc[index]['label']\n",
    "image_path = os.path.join('WW Returns/images', image_id)\n",
    "image = mpimg.imread(image_path)\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(image)\n",
    "plt.title(\"Label: {}\".format(label), fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedb120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aabebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6cbeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Preprocessing\n",
    "# \n",
    "# - Get sample batch and fit it on ImageDataGenerator for zca\n",
    "# - Define train and validation set from pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Pre-trained Xception weights requires that input be scaled\n",
    "# from (0, 255) to a range of (-1., +1.), the rescaling layer\n",
    "def preprocessing_fun(image):\n",
    "    img = np.array(image)\n",
    "    img /= 127.5\n",
    "    img -= 1\n",
    "    return img\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "#     zca_whitening=True,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    preprocessing_function = preprocessing_fun, \n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.2,1.0],\n",
    "    fill_mode='nearest')\n",
    "\n",
    "\n",
    "# Need to fit this since zca_whitening is used\n",
    "# train_sample = datagen.flow_from_dataframe(dataframe=train_data[0:5], \n",
    "#                                               directory='WW Returns/images/',\n",
    "#                                               x_col=\"image_id\", \n",
    "#                                               y_col=\"label\", \n",
    "#                                               class_mode=\"categorical\",\n",
    "#                                                 classes=['missing', 'not-easy-fraud', 'replaced'],\n",
    "#                                               target_size=(150, 150), \n",
    "#                                         color_mode='rgb',\n",
    "#                                               batch_size=5)\n",
    "# for batch in train_sample:\n",
    "#     datagen.fit(batch[0])\n",
    "#     break\n",
    "    \n",
    "    \n",
    "\n",
    "with open('datagen.pb', 'wb') as datagen_file:\n",
    "    pickle.dump(datagen, datagen_file)\n",
    "    \n",
    "\n",
    "with open('datagen.pb', 'rb') as datagen_file:\n",
    "    datagen = pickle.load(datagen_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5ecd7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# There will be 3 text inputs\n",
    "# 1. gl_product: Here we will split by ('_') and remove numbers eg. gl_product_9_digital and remove the first 'gl'\n",
    "# 2. cat_desc: Here - \"1000 Point_&_Shoot\", \"1400 Health & Wellness (121)\" we will remove number and & and (121) and _ and /\n",
    "# 3. subcat_desc: Same as above and DELETED and remove any word that contains numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310895b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def split(string):\n",
    "    delimiters = \",\", \"_\", \"/\", \"&\", \" \"\n",
    "    regexPattern = '|'.join(map(re.escape, delimiters))\n",
    "    return re.split(regexPattern, string)\n",
    "\n",
    "def remove(strList):\n",
    "    # Given list of words, remove/edit some words eg. (121)\n",
    "    # Remove words that contain numbers\n",
    "    # Remove DELETED\n",
    "    # Remove gl\n",
    "    return [i.lower() for i in strList if i.isalpha() and i != 'DELETED' and i != 'gl']\n",
    "\n",
    "def embedWords(listString):\n",
    "    # Either pass one word: \"Computer\" or sentence \"I am a computer\"\n",
    "    return embed(listString)\n",
    "\n",
    "def remove_stopwords(tokenized_list):\n",
    "    # Remove all English Stopwords\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "def stemming(tokenized_text):\n",
    "    ps = nltk.PorterStemmer()\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "def lemmatizing(tokenized_text):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def process_text(row):\n",
    "    gl = row['gl_product_group_desc']\n",
    "    cat = row['cat_desc']\n",
    "    subcat = row['subcat_desc']\n",
    "    \n",
    "#     print(\"GL: \" + gl)\n",
    "    splitGl = split(gl)\n",
    "    finalGl = remove(splitGl)\n",
    "#     print(\"SplitGL: \" + str(splitGl))\n",
    "#     print(\"FinalGL: \" + finalGl)\n",
    "#     print()\n",
    "    \n",
    "#     print(\"Cat: \" + cat)\n",
    "    splitCat = split(cat)\n",
    "    finalCat = remove(splitCat)\n",
    "#     print(\"SpCat: \" + str(splitCat))\n",
    "#     print(\"FCat: \" + finalCat)\n",
    "#     print()\n",
    "    \n",
    "#     print(\"SubCat: \" + subcat)\n",
    "    splitSubcat = split(subcat)\n",
    "    finalSubcat = remove(splitSubcat)\n",
    "#     print(\"Split SubCat: \" + str(splitSubcat))\n",
    "#     print(\"FSubCat: \" + finalSubcat)\n",
    "#     print()\n",
    "    \n",
    "#     print(\"--------------------\\n\")\n",
    "    words = [i for i in finalGl+finalCat+finalSubcat]\n",
    "    words = remove_stopwords(words)\n",
    "    words = stemming(words)\n",
    "    words = lemmatizing(words)\n",
    "#     return embedWords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e244a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ### Get total list of words in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wordList = set()\n",
    "word_samples = []\n",
    "\n",
    "# Train\n",
    "for i,data in train_data.iterrows():\n",
    "    words = process_text(data)\n",
    "    word_samples.append(words)\n",
    "    for word in words:\n",
    "        wordList.add(word)\n",
    "        \n",
    "# Test\n",
    "test_features = pd.read_csv('WW Returns/test_features.csv')\n",
    "for i,data in test_features.iterrows():\n",
    "    words = process_text(data)\n",
    "    word_samples.append(words)\n",
    "    for word in words:\n",
    "        wordList.add(word)\n",
    "        \n",
    "print(len(wordList))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6161bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8bcd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba865474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# !curl http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip --output glove.6B.zip\n",
    "# !unzip glove.6B.zip\n",
    "\n",
    "\n",
    "# ## Initialise glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694df4d7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocabulary_size = len(wordList)\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9d01e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
    "texts = []\n",
    "for sample in word_samples:\n",
    "    texts.append(' '.join(sample))\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e79a23",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f1f80",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# word_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015703b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < vocabulary_size:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f984bb7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf73306",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "train_data = pd.read_csv('dtrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efec6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ### Data Generators: Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_gen = datagen.flow_from_dataframe(dataframe=train_data[:-1000], \n",
    "                                              directory='WW Returns/images/',\n",
    "                                              x_col=\"image_id\", \n",
    "                                              y_col=\"label\", \n",
    "                                              class_mode=\"categorical\",\n",
    "                                                classes=['missing', 'not-easy-fraud', 'replaced'],\n",
    "                                              target_size=(150, 150), \n",
    "                                        color_mode='rgb',\n",
    "                                              batch_size=256)\n",
    "# Classes are encoded in 1 hot vector by alphabetical order\n",
    "\n",
    "\n",
    "val_gen = datagen.flow_from_dataframe(dataframe=train_data[-1000:], \n",
    "                                              directory='WW Returns/images/',\n",
    "                                              x_col=\"image_id\", \n",
    "                                              y_col=\"label\", \n",
    "                                              class_mode=\"categorical\",\n",
    "                                                classes=['missing', 'not-easy-fraud', 'replaced'],\n",
    "                                              target_size=(150, 150), \n",
    "                                      color_mode='rgb',\n",
    "                                              batch_size=256)\n",
    "\n",
    "\n",
    "# ### Data Generators: Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, dataframe, batch_size=32):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.dataframe = dataframe\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.dataframe) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        # indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        dataframe_batch = self.dataframe[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(dataframe_batch)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.dataframe))\n",
    "\n",
    "    def __data_generation(self, dataframe_batch):\n",
    "        'Generates data containing batch_size samples'\n",
    "    \n",
    "        X_images = []\n",
    "        X_text = []\n",
    "        Y = []\n",
    "        for index, data in dataframe_batch.iterrows():\n",
    "            label = data['label']\n",
    "            y = rev_labels_dict[label]\n",
    "            image_id = data['image_id']\n",
    "            \n",
    "            x_text = process_text(data)\n",
    "            x_text = ' '.join(x_text)\n",
    "            sequence = tokenizer.texts_to_sequences([x_text])\n",
    "            x_text_padded = pad_sequences(sequence, maxlen=max_words, padding='post')[0]\n",
    "\n",
    "            image_path = os.path.join('WW Returns', 'images', image_id)\n",
    "            image = cv2.imread(image_path).astype(np.float32)\n",
    "\n",
    "            image = cv2.resize(image, new_shape)\n",
    "            preprocessed_image = preprocessing_fun(image)\n",
    "            X_images.append(preprocessed_image)\n",
    "            X_images.append(preprocessed_image)\n",
    "            Y.append(y)\n",
    "            Y.append(y)\n",
    "            X_text.append(x_text_padded)\n",
    "            X_text.append(x_text_padded)\n",
    "            \n",
    "            \n",
    "\n",
    "            num_of_aug = 1\n",
    "            MAX_NUM_OF_AUG = num_augmentations[label] - 2\n",
    "            for batch in datagen.flow(np.expand_dims(image, axis=0)):\n",
    "                aug_image = batch[0]\n",
    "\n",
    "                X_images.append(aug_image)\n",
    "                Y.append(y)\n",
    "                X_text.append(x_text_padded)\n",
    "\n",
    "                if num_of_aug >= MAX_NUM_OF_AUG:\n",
    "                    break\n",
    "                num_of_aug = num_of_aug+1\n",
    "\n",
    "                \n",
    "        return [np.array(X_images), np.array(X_text)], keras.utils.to_categorical(Y, num_classes=3)\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893573ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74738a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdbe20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1908eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe92be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "base_model = keras.applications.Xception(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False,\n",
    ")  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "# Freeze the base_model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create new model on top\n",
    "image_input = keras.Input(shape=(150, 150, 3), name='image_input')\n",
    "\n",
    "\n",
    "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
    "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "# base_model is running in inference mode here.\n",
    "base_cnn_with_input = base_model(image_input, training=False)\n",
    "cnn_gavg = keras.layers.GlobalAveragePooling2D()(base_cnn_with_input)\n",
    "cnn_drop = keras.layers.Dropout(0.2)(cnn_gavg)  # Regularize with dropout\n",
    "# cnn_dense_32 = keras.layers.Dense(32, activation='relu')(cnn_drop)\n",
    "cnn_dense_16 = keras.layers.Dense(8, activation='relu')(cnn_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3c6e6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word_embeddings = keras.Input(shape=(None,), name='word_embeddings')\n",
    "embedded_tag = keras.layers.Embedding(vocabulary_size, embedding_dim, input_length=max_words)(word_embeddings)\n",
    "\n",
    "encoded_tag = keras.layers.LSTM(128)(embedded_tag)\n",
    "# x_text_layer = keras.layers.Dense(32, activation='relu')(encoded_tag)\n",
    "x_text_layer = keras.layers.Dense(8, activation='relu')(encoded_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a73ce6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "concatenated = keras.layers.concatenate([cnn_dense_16, x_text_layer], axis=-1)\n",
    "# concatenated = keras.layers.Dense(16, activation='relu')(concatenated)\n",
    "\n",
    "outputs = keras.layers.Dense(3, activation='softmax')(concatenated)\n",
    "model = keras.Model([image_input, word_embeddings] , outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c0f0e4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.layers[4].set_weights([embedding_matrix])\n",
    "model.layers[4].trainable = False # freeze GloVe word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0674332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4745e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save JSON config to disk\n",
    "\n",
    "def save_model(iteration=None, model=model):\n",
    "    json_config = model.to_json()\n",
    "    with open('model_config.json', 'w') as json_file:\n",
    "        json_file.write(json_config)\n",
    "    # Save weights to disk\n",
    "    \n",
    "    if iteration:\n",
    "        file_path = 'path_to_my_weights_' + iteration + '.h5'\n",
    "    else:\n",
    "        file_path = 'path_to_my_weights.h5'\n",
    "    file1 = open(file_path,\"w\")\n",
    "    file1.close()\n",
    "    \n",
    "    model.save_weights(\"path_to_my_weights.h5\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65949cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_model():\n",
    "    # Reload the model from the 2 files we saved\n",
    "    with open('model_config.json') as json_file:\n",
    "        json_config = json_file.read()\n",
    "    new_model = keras.models.model_from_json(json_config)\n",
    "    file_path = 'path_to_my_weights.h5'\n",
    "    new_model.load_weights(file_path)\n",
    "    return new_model\n",
    "\n",
    "\n",
    "# # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5546ee0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#%%capture test\n",
    "\n",
    "def test_full(model):\n",
    "    \n",
    "    import string\n",
    "    import random\n",
    "\n",
    "    # initializing size of string \n",
    "    N = 7\n",
    "    # using random.choices()\n",
    "    # generating random strings \n",
    "    res = ''.join(random.choices(string.ascii_uppercase +\n",
    "                                 string.digits, k = N))\n",
    "    file_path = 'predictions_' + res + '.csv'\n",
    "    \n",
    "    # Let's look at the test data we need to make predictions for\n",
    "    test_features = pd.read_csv('WW Returns/test_features.csv')\n",
    "    test_features.head()\n",
    "\n",
    "    # Below is an example submission of a baseline model with directions on how to submit to Leaderboard\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df['ID'] = test_features['ID']\n",
    "\n",
    "    result_df = pd.read_csv('Results/predictions.csv')\n",
    "    result_df['ID'] = test_features['ID']\n",
    "    result_df['label'] = ''\n",
    "\n",
    "\n",
    "\n",
    "    NUM_OF_TEST_AUGMENTATIONS = 10\n",
    "\n",
    "    for index, data in test_features.iterrows():\n",
    "        if (result_df.at[index,'label'] != ''):\n",
    "            continue\n",
    "\n",
    "        image_id = data['image_id']\n",
    "        test_id = data['ID']\n",
    "\n",
    "        print(\"Index: \" + str(index))\n",
    "        print(\"Image_id: \" + str(image_id))\n",
    "        print(\"ID: \" + str(test_id))\n",
    "        \n",
    "        \n",
    "        x_text = process_text(data)\n",
    "        x_text = ' '.join(x_text)\n",
    "        sequence = tokenizer.texts_to_sequences([x_text])\n",
    "        x_text_padded = pad_sequences(sequence, maxlen=max_words, padding='post')[0]\n",
    "\n",
    "\n",
    "        image_path = os.path.join('WW Returns', 'images', image_id)\n",
    "        image = cv2.imread(image_path).astype(np.float32)\n",
    "        image = cv2.resize(image, new_shape)\n",
    "\n",
    "        preprocessed_image = preprocessing_fun(image)\n",
    "        img_arr = [preprocessed_image, preprocessed_image, preprocessed_image]\n",
    "        txt_arr = [x_text_padded, x_text_padded, x_text_padded]\n",
    "\n",
    "        num = 1\n",
    "        for batch in datagen.flow(np.expand_dims(image, axis=0)):\n",
    "            img_arr.append(batch[0])\n",
    "            txt_arr.append(x_text_padded)\n",
    "\n",
    "            if num >= NUM_OF_TEST_AUGMENTATIONS:\n",
    "                break\n",
    "            num = num+1\n",
    "\n",
    "        img_arr = np.array(img_arr)\n",
    "        txt_arr = np.array(txt_arr)\n",
    "        predictions = model.predict([img_arr, txt_arr])\n",
    "\n",
    "        predictions = np.sum(predictions, axis=0)\n",
    "        print(predictions)\n",
    "\n",
    "        label = labels_dict[np.argmax(predictions)]\n",
    "        print(label)\n",
    "        result_df.at[index,'label']=label\n",
    "        print(\"\\n\\n\")\n",
    "        result_df.to_csv(os.path.join('Results', file_path), index = False)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28254f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bbac27",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%capture train\n",
    "BATCH_SIZE=150\n",
    "\n",
    "# Generators\n",
    "train_gen = DataGenerator(train_data[:-1000], batch_size=BATCH_SIZE)\n",
    "val_gen = DataGenerator(train_data[-1000:], batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "def fine_tune(model):\n",
    "    history = model.fit(val_gen, epochs=2, validation_data=val_gen, class_weight=class_weight)\n",
    "    # Fine tuning\n",
    "    # Unfreeze the base model\n",
    "    base_model.trainable = True\n",
    "    model.layers[4].trainable = True\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-5),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.fit(train_gen, epochs=2, validation_data=val_gen, class_weight=class_weight)\n",
    "    base_model.trainable = False\n",
    "    model.layers[4].trainable = False # freeze GloVe word embedding\n",
    "    return model\n",
    "\n",
    "def train(model, resume=False):\n",
    "    if resume:\n",
    "        model = load_model()\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        model = fine_tune(model=model)\n",
    "        test_full(model=model)\n",
    "        model = load_model()\n",
    "        \n",
    "    for i in range(10):\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        for ep in range(5):\n",
    "            history = model.fit(train_gen, epochs=1, validation_data=val_gen, class_weight=class_weight)\n",
    "            model = save_model(iteration=str(i)+'-'+str(ep), model=model)\n",
    "        \n",
    "        model = fine_tune(model=model)\n",
    "        test_full(model=model)\n",
    "        model = load_model()\n",
    "\n",
    "    loss, acc = model.evaluate(val_gen)  # returns loss and metrics\n",
    "    print(\"loss: %.2f\" % loss)\n",
    "    print(\"acc: %.2f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d02480",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train(model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf3f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300933cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00caf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08261d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5419096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1096657",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cc51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c84d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd6d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dcabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf17314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b888ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e637bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55168e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa1a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dfd1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ed5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b10241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d5a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d03250",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
